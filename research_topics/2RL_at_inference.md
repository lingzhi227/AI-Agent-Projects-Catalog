下面这份综述聚焦一件事: 不改神经网络权重, 只在推理时调控可变条件, 通过“更聪明的搜索、更稳的策略选择、更有效的上下文与工具管理”把系统指标顶上去。给你路线图、可用算法、评测与落地工程做法。附上关键来源, 避免纸上谈兵。

# 一句话先定框架

推理期优化可以看成三层叠加

1. 计算换质量: 让模型在一次输入上多想几步、多走几条路、再筛。
2. 策略当一类“策略臂”: 用上下文特征选用不同 prompt、样本数、检索配额、工具调用。在线或离线用带偏反馈做学习。
3. 可验证就闭环: 用你的理想答案当奖励, 让生成和选择变成可度量的搜索问题与带反馈的决策问题。

---

## 1. 纯推理期“计算换质量”的方法族

这些方法不训练权重, 靠更好的搜索与选择提升正确率。

* **Self-Consistency 与 Best-of-N**
  同一问题采样多条思路, 用投票或得分挑最优。经典做法显著提高推理任务表现。可以继续用“自置信”这类无需外部打分器的评分, 更省成本, 在开放题也稳。

* **Tree-of-Thoughts 与基于搜索的推理**
  把思路分叉成树, 用 BFS 或 DFS 探索, 配上自评或外部验证做剪枝。适合需要规划与回溯的问题。

* **RAP 与 MCTS 导航**
  把 LLM当世界模型与行动体, 用蒙特卡洛树搜索在思路空间里“看前看后”。可把你的奖励函数接进来当 rollout 终止的分数。

* **反思与自我修正**
  先答, 再让模型自我复盘, 生成修正指令后重答。多数数据集下有效, 代价是时延与令牌。

**工程要点**
设一个统一的“候选生成器”与“候选评审器”。生成器负责采样、ToT、或MCTS。评审器可以是规则、单元测试、字符串匹配、评分器模型、或“自置信”。最后一层是选择与早停, 比如当第一名和第二名的分差有统计把握就停。上述三层可按任务开启或关闭。

---

## 2. 推理期“策略选择”的学习化

把“怎么推理”本身当决策问题。不同 prompt、样本数 N、温度、工具是否调用、检索条数……都看作“策略臂”。

* **多臂与情境多臂**
  在线用 UCB 或汤普森采样在不同策略间探索与利用。也可以做**模型路由**或**成本感知路由**, 例如选便宜模型先试, 不过线再升级。近期工作把 LLM 选择与策略选择系统化成 bandit, 有理论与实证。

* **离线带偏反馈学习, 可免去线上试错成本**
  用你现有的“问题, 答案, 系统策略, 得分”的日志, 做**反事实评估与学习**。经典做法是 IPS 和 Doubly Robust 估计器, 以及 Counterfactual Risk Minimization, 在推荐与检索里已很成熟, 直接可复用到提示词和策略选择上。

* **策略内的“子策略”选择**
  比如在进化式提示优化中, 用 bandit 在不同编辑策略间切换会更稳。

**工程要点**
定义上下文特征, 如问题长度与类型、是否可用外部校验、历史成功率、检索命中率、预计 token 成本、时延预算。训练一个**情境多臂**来选择策略臂。离线先用 IPS 或 DR 做评估与学习, 上线后小流量继续探索更新参数。

---

## 3. 上下文与检索的推理期优化

* **示例检索与排序作为 RL/序贯决策**
  RetICL 把 few-shot 示例选择建成 MDP, 用 RL 顺序挑例子并考虑顺序影响。你的用例里可以把“示例池来自你数据集”, 奖励就是对照理想答案的得分。

* **RAG 的“足够上下文”与门控**
  先用“足够上下文”判定器或轻量排序模块判断是否需要检索与要多少文本, 再决定是否调用工具或加深搜索。苹果与谷歌的实践提示, 做好上下文门控能明显降幻觉与成本。

* **把上下文配额当背包或次模选择**
  工程上常把每段文档估一个“信息增益分数/成本”, 做贪心或近似背包。若你要“会学”, 可把选择策略并入前述 bandit。

---

## 4. 工具调用与代理评测

* **ReAct 风格推理加行动**
  推理与动作交替, 适合需要查库与 API 的任务, 你的策略层负责“何时调用”。
* **专用基准与可靠性指标**
  τ-bench 和 AgentBench 给了多回合、工具、合规等维度的评估, 还有 pass^k 这类稳定性指标。把它们接入你的离线回放。
* **关于“用 RL 学工具”**
  近期论文强调直接用奖励学工具能力, 但那是训练期话题。你不改权重的前提下, 借鉴的是奖励设计与分解思想, 把它放到推理期的评分与搜索里。

---

## 5. 用外部与内部验证器来“选优而非训练”

* **外部验证器**
  过程或结果级验证器给每条思路打分, 用于 BoN 选择或 ToT 剪枝。经典“逐步验证”工作系统地展示了过程验证的价值。
* **LLM 评审与自动指标**
  G-Eval, JudgeLM 一类做评审的模型可以作为候选评分器。报告里也有提示, 这类评审有偏与不稳定, 小心使用, 最好与可编程测试并用。

---

## 6. 把这些拼成你的落地方案

你手里正好有数据集与理想答案, 这让闭环极其直接。

**步骤 A. 定义奖励**
任务是抽取就用 EM 或 F1, 是生成就用 ROUGE 或自定义打分器, 代码题用单元测试, 结构化题用执行型验证器。评分要可重复与快速。

**步骤 B. 列出策略空间**
prompt 模板若干, N 与温度, ToT 的分支与深度, 是否调用工具与顺序, RAG 的检索条数与重排器, 以及是否启用自反思。每个组合是一条“策略臂”。

**步骤 C. 先离线, 再在线**

1. 复用历史日志或批量离线跑各策略臂, 得到“问题, 策略臂, 奖励, 概率”日志。
2. 用 IPS 或 Doubly Robust 做反事实评估与选型, 也可以直接做 CRM 学一个情境多臂策略。工具现成, 比如 Vowpal Wabbit 的 OPE 教程和 Open Bandit Pipeline。
3. 上线用小流量加 UCB 或汤普森继续学习与保守探索。新样本持续进仓, 每天或每周再离线重估一次策略。

**步骤 D. 候选生成与选择统一接口**

* 生成器: Greedy, BoN, ToT, MCTS。
* 评审器: 你的奖励函数, 单测, 验证器, 自置信。
* 选择器: 规则或学习到的线性打分。必要时做早停与动态 N, 当第一名领先幅度达阈值就停, 节省令牌。

**步骤 E. 上下文与工具门控**

* 先跑“足够上下文”判定与轻量排序。
* 判定不足再检索, 不足再调工具。把这三步的阈值也放进策略臂并让 bandit 学。

**步骤 F. 评测面板**
准确率、pass@k、平均成本、时延、失败类型分布、策略臂占比与后悔度, 再加稳定性指标 pass^k。

---

## 7. 你可以直接复用的算法和组件

* **ToT 与 RAP 实现**, 便于接你的评分器。
* **Prompt 与策略优化**, DSPy 的“编译器式”优化能把你的管线模块化, 不强制训练权重, 易于接你定义的指标。PromptAgent 与进化式方法负责离线找候选模板。
* **Bandit 与 OPE 工具链**, VW 的 OPE 教程与 OBP 基准。
* **代理评测**, τ-bench 与 AgentBench。

---

## 8. 常见陷阱与预案

* **评审器被“讨好”**
  模型学会写出迎合评审器的答案格式, 但事实错。解决: 用可执行验证优先于文本评审, 多评审器投票, 对评审器做小样本人工抽检。
* **离线学习的高方差**
  IPS 会抖。解决: 用 DR 或 SWITCH, 控方差, 再配合模型化打分器。
* **成本与时延失控**
  ToT 与 BoN 很烧钱。解决: 动态 N 与早停, 自置信或外部验证触发停机。
* **策略外推失败**
  线下赢家到线上翻车。解决: 上线用保护带, 逐步放量, 并行保留强基线策略臂作回退。
* **检索污染与泄漏**
  用理想答案做奖励时避免泄漏到模型上下文。严格隔离“评测用答案”和“可见上下文”。

---

## 9. 快速蓝图, 直接可做

1. 用你数据集的理想答案实现任务奖励与可执行校验。
2. 选 5 到 10 条风格差异大的 prompt 模板, 再加两个开关: 是否启用 ToT 与是否调用工具。
3. 批量离线跑每条策略臂, 收集策略日志。
4. 用 DR 做反事实评估, 训练一个情境多臂策略路由。
5. 上线 10% 流量用汤普森采样, 其余走离线最优臂, 设好早停与成本阈值。
6. 每天回放新日志更新路由, 每周重算候选模板, 借助 DSPy 或 PromptAgent 生成新候选再入臂池。

---

### 参考与延伸

* 计算换质量: Self-Consistency, ToT, RAP, 自置信选择, 自反思。
* 策略选择与路由: LLM-Bandit, 上下文 bandit, 策略内 bandit。
* 上下文与检索: RetICL, 足够上下文与排序。
* 工具与评测: ReAct, τ-bench, AgentBench。
* 反事实评估与学习: CRM, IPS, DR, VW OPE, OBP。

如果你愿意, 我可以按你的数据集类型, 直接把“策略臂清单、奖励函数、bandit 路由特征表、以及 ToT 或 MCTS 的默认超参”写成一份最小可用配置, 你拿去跑就行。
